---
layout: page
title:  "How to configure a highly-available IP"
tags: [neutron, configuring ip, highly available]
date: April 9th, 2015
author: Eric French
featured: false
weight: 4
---

This article provides a resource on how to set up HA floating IPs within an **IBM Blue Box Cloud OpenStack** environment. These instructions assume that you already have the **OpenStack Python API Clients** installed.

**Configure your OpenStack instances**

There are two portions to the OpenStack work. The first portion requires administrative credentials to set up availability zones, and the second portion can be done by a regular user. The first portion only needs to be done once.

###First Portion: Administrative steps

**Set up availability zones**

We use availability zones to create a pool of resources to ensure that our two unique VMs don't end up on the same physical hosts.

As an administrator, create two distinct availability zones:

{% highlight bash %}
$ nova aggregate-create group1 group1-az
+----+--------+-------------------+-------+-------------------------------+
| Id | Name   | Availability Zone | Hosts | Metadata                      |
+----+--------+-------------------+-------+-------------------------------+
| 8  | group1 | group1-az         |       | 'availability_zone=group1-az' |
+----+--------+-------------------+-------+-------------------------------+
$ nova aggregate-create group2 group2-az
+----+--------+-------------------+-------+-------------------------------+
| Id | Name   | Availability Zone | Hosts | Metadata                      |
+----+--------+-------------------+-------+-------------------------------+
| 11 | group2 | group2-az         |       | 'availability_zone=group2-az' |
+----+--------+-------------------+-------+-------------------------------+
{% endhighlight %}

View your availability zone information to get information on your hypervisors:

{% highlight bash %}
$ nova availability-zone-list
+-----------------------+----------------------------------------+
| Name                  | Status                                 |
+-----------------------+----------------------------------------+
| internal              | available                              |
| |- ds1263             |                                        |
| | |- nova-conductor   | enabled :-) 2014-12-03T00:07:33.000000 |
| | |- nova-cert        | enabled :-) 2014-12-03T00:07:33.000000 |
| | |- nova-consoleauth | enabled :-) 2014-12-03T00:07:33.000000 |
| | |- nova-scheduler   | enabled :-) 2014-12-03T00:07:27.000000 |
| |- ds1264             |                                        |
| | |- nova-conductor   | enabled :-) 2014-12-03T00:07:32.000000 |
| | |- nova-cert        | enabled :-) 2014-12-03T00:07:32.000000 |
| | |- nova-consoleauth | enabled :-) 2014-12-03T00:07:33.000000 |
| | |- nova-scheduler   | enabled :-) 2014-12-03T00:07:30.000000 |
| group1-az             | available                              |
| |- ds1263             |                                        |
| | |- nova-compute     | enabled :-) 2014-12-03T00:07:33.000000 |
| group2-az             | available                              |
| |- ds1264             |                                        |
| | |- nova-compute     | enabled :-) 2014-12-03T00:07:33.000000 |
| nova                  | available                              |
| |- ds1265             |                                        |
| | |- nova-compute     | enabled :-) 2014-12-03T00:07:32.000000 |
| |- ds1266             |                                        |
| | |- nova-compute     | enabled :-) 2014-12-03T00:07:32.000000 |
+-----------------------+----------------------------------------+
{% endhighlight %}

Selecting hosts from that list, add them to your two distinct availability zones:

{% highlight bash %}
$ nova aggregate-add-host 8 ds1263
Host ds1263 has been successfully added for aggregate 8
+----+--------+-------------------+----------+-------------------------------+
| Id | Name   | Availability Zone | Hosts    | Metadata                      |
+----+--------+-------------------+----------+-------------------------------+
| 8  | group1 | group1-az         | 'ds1263' | 'availability_zone=group1-az' |
+----+--------+-------------------+----------+-------------------------------+
$ nova aggregate-add-host 11 ds1264
Host ds1264 has been successfully added for aggregate 11
+----+--------+-------------------+----------+-------------------------------+
| Id | Name   | Availability Zone | Hosts    | Metadata                      |
+----+--------+-------------------+----------+-------------------------------+
| 11 | group2 | group2-az         | 'ds1264' | 'availability_zone=group2-az' |
+----+--------+-------------------+----------+-------------------------------+
{% endhighlight %}

That's it!

###Second Portion: Steps That Require Regular Permissions Only

* Confirm that the `allowed-address-pairs` extension is loaded.

The `allowed-address-pairs` extension is required, and it is shipped by default with each **IBM Blue Box** Cloud installation. You can confirm it's operating by running the following commands:

{% highlight bash %}
$ neutron ext-list | grep allowed-address-pairs
| allowed-address-pairs | Allowed Address Pairs
|
{% endhighlight %}

* Create a VRRP security group.

This security group is used to communicate to your HA nodes. In this instance, we're permitting `SSH` access, but you can make this protocol `HTTP`, `HTTPS`, etc...

{% highlight bash %}
$ neutron security-group-create vrrp
Created a new security_group:
+----------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Field                | Value                                                                                                                                                                                                                                                                                                                         |
+----------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| description          |                                                                                                                                                                                                                                                                                                                               |
| id                   | 7f09ac66-23e2-46cd-9af8-ca541dd99bc1                                                                                                                                                                                                                                                                                          |
| name                 | vrrp                                                                                                                                                                                                                                                                                                                      |
| security_group_rules | {"remote_group_id": null, "direction": "egress", "remote_ip_prefix": null, "protocol": null, "tenant_id": "0109935df2314105bf7ba85a79a1c001", "port_range_max": null, "security_group_id": "7f09ac66-23e2-46cd-9af8-ca541dd99bc1", "port_range_min": null, "ethertype": "IPv4", "id": "28236695-9cb4-4a0c-a04d-9d230fcdce4a"} |
|                      | {"remote_group_id": null, "direction": "egress", "remote_ip_prefix": null, "protocol": null, "tenant_id": "0109935df2314105bf7ba85a79a1c001", "port_range_max": null, "security_group_id": "7f09ac66-23e2-46cd-9af8-ca541dd99bc1", "port_range_min": null, "ethertype": "IPv6", "id": "a8180e54-6ae6-44e7-9da8-baeec24dbe6f"} |
| tenant_id            | 0109935df2314105bf7ba85a79a1c001                                                                                                                                                                                                                                                                                              |
+----------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
{% endhighlight %}

* Add `ICMP` access:

{% highlight bash %}
$ $ neutron security-group-rule-create --protocol icmp vrrp
Created a new security_group_rule:
+-------------------+--------------------------------------+
| Field             | Value                                |
+-------------------+--------------------------------------+
| direction         | ingress                              |
| ethertype         | IPv4                                 |
| id                | f795c776-2e8c-4234-9f9c-cff5bd4398fc |
| port_range_max    |                                      |
| port_range_min    |                                      |
| protocol          | icmp                                 |
| remote_group_id   |                                      |
| remote_ip_prefix  |                                      |
| security_group_id | 7f09ac66-23e2-46cd-9af8-ca541dd99bc1 |
| tenant_id         | 0109935df2314105bf7ba85a79a1c001     |
+-------------------+--------------------------------------+
{% endhighlight %}

* Add `SSH` access:

{% highlight bash %}
$ neutron security-group-rule-create --protocol tcp --port-range-min 22 --port-range-max 22 vrrp
Created a new security_group_rule:
+-------------------+--------------------------------------+
| Field             | Value                                |
+-------------------+--------------------------------------+
| direction         | ingress                              |
| ethertype         | IPv4                                 |
| id                | 41c996f5-1e7b-4061-87ff-8dba581eda7c |
| port_range_max    | 22                                   |
| port_range_min    | 22                                   |
| protocol          | tcp                                  |
| remote_group_id   |                                      |
| remote_ip_prefix  |                                      |
| security_group_id | 7f09ac66-23e2-46cd-9af8-ca541dd99bc1 |
| tenant_id         | 0109935df2314105bf7ba85a79a1c001     |
+-------------------+--------------------------------------+
{% endhighlight %}

* Collect parameters required to boot the instances.

You'll need to collect a few specific pieces of information to ensure that our HA load balancers are provisioned with the OS we want, and that they end up as unique physical hypervisors.

* Select a particular flavor ID. In this case, we will use `m1.medium` with ID `3`:

{% highlight bash %}
$ nova flavor-list
+----+-----------+-----------+------+-----------+---------+-------+-------------+-----------+
| ID | Name      | Memory_MB | Disk | Ephemeral | Swap_MB | VCPUs | RXTX_Factor | Is_Public |
+----+-----------+-----------+------+-----------+---------+-------+-------------+-----------+
| 1  | m1.tiny   | 512       | 10   | 0         |         | 1     | 1.0         | True      |
| 2  | m1.small  | 2048      | 20   | 0         |         | 1     | 1.0         | True      |
| 3  | m1.medium | 4096      | 40   | 0         |         | 2     | 1.0         | True      |
| 4  | m1.large  | 8192      | 80   | 0         |         | 4     | 1.0         | True      |
| 5  | m1.xlarge | 16384     | 160  | 0         |         | 8     | 1.0         | True      |
+----+-----------+-----------+------+-----------+---------+-------+-------------+-----------+
{% endhighlight %}

* Select the ID of the particular image. In this case, we've selected `ubuntu-14.04` and the ID is `1525c3f3-1224-4958-bd07-da9feaedf18b`:

{% highlight bash %}
$ nova image-list | grep  ubuntu-14.04
+--------------------------------------+---------------------------------+--------+--------+
| ID                                   | Name                            | Status | Server |
+--------------------------------------+---------------------------------+--------+--------+
| 1525c3f3-1224-4958-bd07-da9feaedf18b | ubuntu-14.04                    | ACTIVE |        |
+--------------------------------------+---------------------------------+--------+--------+
{% endhighlight %}

* Collect the ID of the Internal network. In this case, that is `ba0fdd03-72b5-41eb-bb67-fef437fd6cb4`:

{% highlight bash %}
$ nova network-list
+--------------------------------------+----------+------+
| ID                                   | Label    | Cidr |
+--------------------------------------+----------+------+
| 06dc9d5a-f55a-410d-a7fd-4c7cb34ad927 | external | -    |
| ba0fdd03-72b5-41eb-bb67-fef437fd6cb4 | internal | -    |
+--------------------------------------+----------+------+
{% endhighlight %}

* Select the IDs of the `default` and `VRRP` groups, in this case they are `a963c698-473c-4182-8add-e1e6398629e5` and `7f09ac66-23e2-46cd-9af8-ca541dd99bc1` respectively:

{% highlight bash %}
$ nova secgroup-list
+--------------------------------------+----------+-------------+
| Id                                   | Name     | Description |
+--------------------------------------+----------+-------------+
| a963c698-473c-4182-8add-e1e6398629e5 | default  | default     |
| 7f09ac66-23e2-46cd-9af8-ca541dd99bc1 | vrrp |             |
+--------------------------------------+----------+-------------+
{% endhighlight %}

* Select the two unique availability zones we created previously. In this case, `group1-az` and `group2-az`.

{% highlight bash %}
$ nova availability-zone-list
+-----------+-----------+
| Name      | Status    |
+-----------+-----------+
| group1-az | available |
| group2-az | available |
| nova      | available |
+-----------+-----------+
$
{% endhighlight %}

* Boot two unique instances on two unique hosts.

Here's a synopsis of the values we collected from before:

{% highlight bash %}
flavor: 3
image: 1525c3f3-1224-4958-bd07-da9feaedf18b
availability-zone: group1-az, group2-az
security groups: b3ccd3e2-6566-40ba-8469-9919a44c0eaf, 1409d05d-7b34-4f77-8ae0-8c49c6be37a2
network id: ba0fdd03-72b5-41eb-bb67-fef437fd6cb4
{% endhighlight %}

Using those variables, let's boot our two VMs.

**Additional notes:**

You'll need to specify the `key-name` for the `SSH` key you use in the system. For example:

**Host 1**:

{% highlight bash %}
$ nova boot \
--key-name default \
--flavor 3 \
--image 1525c3f3-1224-4958-bd07-da9feaedf18b \
--availability-zone group1-az \
--security-groups a963c698-473c-4182-8add-e1e6398629e5,7f09ac66-23e2-46cd-9af8-ca541dd99bc1 \
--nic net-id=ba0fdd03-72b5-41eb-bb67-fef437fd6cb4 \
loadbalancer01
{% endhighlight %}

**Host 2**: (Notice that we're changing the availability zone and the hostname.)

{% highlight bash %}
$ nova boot \
--key-name default \
--flavor 3 \
--image 1525c3f3-1224-4958-bd07-da9feaedf18b \
--availability-zone group2-az \
--security-groups a963c698-473c-4182-8add-e1e6398629e5,7f09ac66-23e2-46cd-9af8-ca541dd99bc1 \
--nic net-id=ba0fdd03-72b5-41eb-bb67-fef437fd6cb4 \
loadbalancer02
{% endhighlight %}

* Collect instance data.

Now that your two instances have booted, you can collect the instance data.

{% highlight bash %}
$ nova list
+--------------------------------------+----------------+--------+------------+-------------+-----------------------+
| ID                                   | Name           | Status | Task State | Power State | Networks              |
+--------------------------------------+----------------+--------+------------+-------------+-----------------------+
| 9d64868e-55ae-4b40-b2fa-70a65b0e88e8 | loadbalancer01 | ACTIVE | -          | Running     | internal=10.230.7.183 |
| 0394331a-f2ce-480a-9a93-2a50207ef7ee | loadbalancer02 | ACTIVE | -          | Running     | internal=10.230.7.184 |
+--------------------------------------+----------------+--------+------------+-------------+-----------------------+
{% endhighlight %}

* Create a port.

**NOTE**: This action requires administrative credentials, but it can be executed to add the port to a specific tenant. If you want this port to be added to a tenant, use the `--tenant-id` parameter.

Also, be sure to make note of the IP address and the port id that's returned. In this case, it is `10.230.7.33` and `6bed4a2a-62e6-4cb1-80ce-b5971fee782d`.

{% highlight bash %}
$ neutron port-create --security-group 7f09ac66-23e2-46cd-9af8-ca541dd99bc1 internal
Created a new port:
+-----------------------+------------------------------------------------------------------------------------+
| Field                 | Value                                                                              |
+-----------------------+------------------------------------------------------------------------------------+
| admin_state_up        | True                                                                               |
| allowed_address_pairs |                                                                                    |
| binding:capabilities  | {"port_filter": true}                                                              |
| binding:host_id       |                                                                                    |
| binding:vif_type      | ovs                                                                                |
| device_id             |                                                                                    |
| device_owner          |                                                                                    |
| fixed_ips             | {"subnet_id": "8fb66dc4-d9f8-47eb-9679-85d7672ad37e", "ip_address": "10.230.7.33"} |
| id                    | 6bed4a2a-62e6-4cb1-80ce-b5971fee782d                                               |
| mac_address           | fa:16:3e:20:c6:e3                                                                  |
| name                  |                                                                                    |
| network_id            | ba0fdd03-72b5-41eb-bb67-fef437fd6cb4                                               |
| security_groups       | 7f09ac66-23e2-46cd-9af8-ca541dd99bc1                                               |
| status                | DOWN                                                                               |
| tenant_id             | 0109935df2314105bf7ba85a79a1c001                                                   |
+-----------------------+------------------------------------------------------------------------------------+
{% endhighlight %}

* If this is a public service, associate a floating IP to the VIP.

Using the `port-id` obtained in the previous step, now we can associate the floating IP to the VIP:

{% highlight bash %}
$ neutron floatingip-create --port-id=6bed4a2a-62e6-4cb1-80ce-b5971fee782d external
Created a new floatingip:
+---------------------+--------------------------------------+
| Field               | Value                                |
+---------------------+--------------------------------------+
| fixed_ip_address    | 10.230.7.33                          |
| floating_ip_address | 173.247.105.96                       |
| floating_network_id | 06dc9d5a-f55a-410d-a7fd-4c7cb34ad927 |
| id                  | 753b3d69-73e4-4396-a98c-df1afdc6c2bf |
| port_id             | 6bed4a2a-62e6-4cb1-80ce-b5971fee782d |
| router_id           | 7e027195-f474-41d7-b15b-9042eb396735 |
| tenant_id           | 0109935df2314105bf7ba85a79a1c001     |
+---------------------+--------------------------------------+
{% endhighlight %}

* Determine the port IDs for your load balancer instances.

Use the IP addresses to map the instances to their ports.

In this case:

`loadbalancer01` -> Port id: `23604992-c26c-426d-9634-f45d24b9ee83`

`loadbalancer02` -> Port id: `9f646d79-0599-4761-a4b3-cb9728bbb72a`

{% highlight bash %}
$ nova list
+--------------------------------------+----------------+--------+------------+-------------+-----------------------+
| ID                                   | Name           | Status | Task State | Power State | Networks              |
+--------------------------------------+----------------+--------+------------+-------------+-----------------------+
| 9d64868e-55ae-4b40-b2fa-70a65b0e88e8 | loadbalancer01 | ACTIVE | -          | Running     | internal=10.230.7.183 |
| 0394331a-f2ce-480a-9a93-2a50207ef7ee | loadbalancer02 | ACTIVE | -          | Running     | internal=10.230.7.184 |
+--------------------------------------+----------------+--------+------------+-------------+-----------------------+
$ neutron port-list
+--------------------------------------+------+-------------------+-------------------------------------------------------------------------------------+
| id                                   | name | mac_address       | fixed_ips                                                                           |
+--------------------------------------+------+-------------------+-------------------------------------------------------------------------------------+
| 23604992-c26c-426d-9634-f45d24b9ee83 |      | fa:16:3e:4d:89:4d | {"subnet_id": "8fb66dc4-d9f8-47eb-9679-85d7672ad37e", "ip_address": "10.230.7.183"} |
| 6bed4a2a-62e6-4cb1-80ce-b5971fee782d |      | fa:16:3e:20:c6:e3 | {"subnet_id": "8fb66dc4-d9f8-47eb-9679-85d7672ad37e", "ip_address": "10.230.7.33"}  |
| 9f646d79-0599-4761-a4b3-cb9728bbb72a |      | fa:16:3e:8d:0e:13 | {"subnet_id": "8fb66dc4-d9f8-47eb-9679-85d7672ad37e", "ip_address": "10.230.7.184"} |
+--------------------------------------+------+-------------------+-------------------------------------------------------------------------------------+
{% endhighlight %}

* Add the new VIP to the allowed address pairs for the instance ports

{% highlight bash %}
$ neutron port-update 23604992-c26c-426d-9634-f45d24b9ee83 --allowed_address_pairs list=true type=dict ip_address=10.230.7.33
Updated port: 23604992-c26c-426d-9634-f45d24b9ee83
$ neutron port-update 9f646d79-0599-4761-a4b3-cb9728bbb72a --allowed_address_pairs list=true type=dict ip_address=10.230.7.33
Updated port: 9f646d79-0599-4761-a4b3-cb9728bbb72a
{% endhighlight %}

**Keepalived**

Now we will move along to the `keepalived` portion of this work, shown partly as sample code that follows.


## Install keepalived


On your two hosts, install `keepalived`:
{% highlight bash %}
$ apt-get install keepalived
{% endhighlight %}


## Configure keepalived on both hosts

**Note**: *Set the password to a unique string.*

The IP listed in the virtual IP address should be your VIP **Host 1**:

{% highlight bash %}
vrrp_instance vrrp_group_1 { state MASTER interface eth0 virtual_router_id 1 priority 100 authentication
    { auth_type PASS auth_pass 9B1D715906B57DF2D78B8D3F02634271CDCC2A51826293FB8E893E21244F7242 }
        virtual_ipaddress { 10.230.7.33/24 brd 10.230.7.255 dev eth0 } }
{% endhighlight %}

**Host 2**:

{% highlight bash %}
vrrp_instance vrrp_group_1 { state BACKUP interface eth0 virtual_router_id 1 priority 50 authentication
    { auth_type PASS auth_pass 9B1D715906B57DF2D78B8D3F02634271CDCC2A51826293FB8E893E21244F7242 }
        virtual_ipaddress { 10.230.7.33/24 brd 10.230.7.255 dev eth0 }

# Start keepalived on both hosts
{% endhighlight %}

**Run the following commands on both hosts:**

{% highlight bash %}
/etc/init.d/keepalived start

## Test the floating IP
{% endhighlight %}

**Try connecting to your public floating IP:**

{% highlight bash %}
$ ssh ubuntu@173.247.105.96 Welcome to Ubuntu 14.04.1 LTS (GNU/Linux 3.13.0-36-generic x86_64)

## Test failover
{% endhighlight %}

**Stop the first load balancer and confirm that the IP fails over.**

{% highlight bash %}
$ nova stop loadbalancer01
$ ssh ubuntu@173.247.105.96
{% endhighlight %}

This command sequence should give you an `SSH key failure` result, because you're now connecting to the second host.

{% highlight bash %}
$ neutron subnet-update internal --dns_nameservers list=true 10.0.100.100
{% endhighlight %}

If **Active Directory** is not deployed, you can use a public DNS server, such as:

{% highlight bash %}
$ neutron subnet-update internal --dns_nameservers list=true 8.8.8.8
{% endhighlight %}
